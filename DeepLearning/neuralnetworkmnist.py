# -*- coding: utf-8 -*-
"""NeuralNetworkMNIST.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ukpd2y8tkOk9yv1QD-VRBaiVoNnkxOkH
"""

import keras
import numpy as np
from keras.datasets import mnist
import time
import tensorflow as tf

(x_train,y_train),(x_test,y_test) = mnist.load_data()
x_train = x_train.reshape(x_train.shape[0],784)/255
x_test = x_test.reshape(x_test.shape[0],784)/255

y_train = keras.utils.np_utils.to_categorical(y_train,10)
y_test = keras.utils.np_utils.to_categorical(y_test,10)

print(x_train.shape)
print(x_test.shape)

class NN():
    def __init__(self, sizes, epochs=10, l_rate=0.001):
        self.sizes = sizes
        self.epochs = epochs
        self.l_rate = l_rate

        # we save all parameters in the neural network in this dictionary
        self.params = self.initialization()

    def sigmoid(self, x, derivation=False):
        if derivation:
            return (np.exp(-x))/((np.exp(-x)+1)**2)
        return 1/(1 + np.exp(-x))

    def softmax(self, x, derivation=False):
        local_exp = np.exp(x - x.max())
        if derivation:
            return local_exp / np.sum(local_exp, axis=0) * (1 - local_exp / np.sum(local_exp, axis=0))
        return local_exp / np.sum(local_exp, axis=0)

    def initialization(self):
        input_layer=self.sizes[0]
        hidden_1=self.sizes[1]
        hidden_2=self.sizes[2]
        output_layer=self.sizes[3]

        params = {
            'W1':np.random.randn(hidden_1, input_layer) * np.sqrt(1. / hidden_1),
            'W2':np.random.randn(hidden_2, hidden_1) * np.sqrt(1. / hidden_2),
            'W3':np.random.randn(output_layer, hidden_2) * np.sqrt(1. / output_layer)
        }

        return params

    def forward_propagation(self, x_train):
        params = self.params

        # input layer activations
        params['A0'] = x_train

        # input layer to hidden layer 1
        params['Z1'] = np.dot(params["W1"], params['A0'])
        params['A1'] = self.sigmoid(params['Z1'])

        # hidden layer 1 to hidden layer 2
        params['Z2'] = np.dot(params["W2"], params['A1'])
        params['A2'] = self.sigmoid(params['Z2'])

        # hidden layer 2 to output layer
        params['Z3'] = np.dot(params["W3"], params['A2'])
        params['A3'] = self.softmax(params['Z3'])

        return params['A3']

    def backward_propagation(self, y_train, output):
        params = self.params
        change_w = {}

        # Calculate W3 update
        error = 2 * (output - y_train) / output.shape[0] * self.softmax(params['Z3'], derivation=True)
        change_w['W3'] = np.outer(error, params['A2'])

        # Calculate W2 update
        error = np.dot(params['W3'].T, error) * self.sigmoid(params['Z2'], derivation=True)
        change_w['W2'] = np.outer(error, params['A1'])

        # Calculate W1 update
        error = np.dot(params['W2'].T, error) * self.sigmoid(params['Z1'], derivation=True)
        change_w['W1'] = np.outer(error, params['A0'])

        return change_w

    def update_parameters(self, changes_to_weights):
        for key, value in changes_to_weights.items():
            self.params[key] -= self.l_rate * value

    def compute_accuracy(self, x_val, y_val):
        predictions = []
        for x, y in zip(x_val, y_val):
            output = self.forward_propagation(x)
            pred = np.argmax(output)
            predictions.append(pred == np.argmax(y))
        return np.mean(predictions)

    def train(self, x_train, y_train, x_val, y_val):
        start_time = time.time()
        for iteration in range(self.epochs):
            for x,y in zip(x_train, y_train):
                output = self.forward_propagation(x)
                changes_to_weights = self.backward_propagation(y, output)
                self.update_parameters(changes_to_weights)
            accuracy = self.compute_accuracy(x_val, y_val)
            print('Epoch: {0}, Time Spent: {1:.2f}s, Accuracy: {2:.2f}%'.format(
                iteration+1, time.time() - start_time, accuracy * 100
            ))

#Comparing to Keras

dnn = NN(sizes=[784, 128, 64, 10])
dnn.train(x_train, y_train, x_test, y_test)

model = keras.Sequential([keras.layers.Dense(1,input_dim=784,activation='sigmoid',name='layer1'),
                          keras.layers.Dense(128,activation='sigmoid',name='layer2'),
                          keras.layers.Dense(64,input_dim=64,activation='sigmoid',name='layer3')])
model.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.SGD(learning_rate=1.0))

